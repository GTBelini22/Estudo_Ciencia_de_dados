{"cells":[{"cell_type":"markdown","metadata":{"id":"jPO7SOoGfQD7"},"source":["<font size=\"6\"><center>Centro Universitário Facens</center></font>\n","\n","<font size=\"4\"><center><b>Disciplina: Inteligência Artificial</b></center></font>\n","  \n","<font size=\"3\"><center>Prof. Dr. Renato Moraes Silva</center></font>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"sTLfb9k3fQD_"},"source":["# <center>Técnicas tradicionais de representação de texto</center>\n","\n","Neste exercício, usaremos técnicas tradicionais para representar os textos de uma base de dados.\n","\n","### Objetivos de aprendizagem\n","- Entender como fazer o pré-processamento de textos\n","- Implementar as principais técnicas de representação de texto\n","- Entender como calcular similaridade de coseno\n","\n","\n","Primeiro, vamos importar as bibliotecas que serão usadas neste exercício."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3Ozg5N6fQEA"},"outputs":[],"source":["import numpy as np # biblioteca usada para trabalhar com vetores e matrizes\n","import pandas as pd # biblioteca usada para trabalhar com dataframes e análise de dados\n","import re # biblioteca para expressoes regulares\n","import os # biblioteca usada para realizar tarefas específicas ao SO\n","\n","from zipfile import ZipFile # biblioteca para arquivos zipados"]},{"cell_type":"markdown","metadata":{"id":"uUx8_dI-fQEC"},"source":["Vamos baixar uma base de dados."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxWxBg1cfQEC","executionInfo":{"status":"ok","timestamp":1709774335282,"user_tz":180,"elapsed":251,"user":{"displayName":"Renato Moraes Silva","userId":"07624830346914349358"}},"outputId":"94401e75-882f-4618-c601-46c989753634"},"outputs":[{"output_type":"stream","name":"stdout","text":["Arquivos extraídos com sucesso!\n"]}],"source":["import os\n","from zipfile import ZipFile # biblioteca para arquivos zipados\n","\n","url = 'https://www.dropbox.com/scl/fi/qnlwhpcqsgoy0m8y23mgz/porsimplessent-master.zip?rlkey=5lb3au2irlz4uv9d6tkbmza0j&dl=0'\n","\n","# especifica o local onde ficarao os arquivos\n","pathFiles = 'dados/'\n","fileName = 'porsimplessent-master.zip'\n","\n","# cria uma pasta onde ficarao os arquivos\n","if not os.path.isdir(pathFiles):\n","    os.mkdir(pathFiles)\n","\n","# faz o download do arquivo\n","os.system('wget -O %s%s %s' %(pathFiles, fileName, url))\n","\n","print('Arquivos extraídos com sucesso!')"]},{"cell_type":"markdown","metadata":{"id":"9-5h3CXHfQED"},"source":["Vamos importar os dados da base."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-V3T8sUfQED","executionInfo":{"status":"ok","timestamp":1709774799592,"user_tz":180,"elapsed":25,"user":{"displayName":"Renato Moraes Silva","userId":"07624830346914349358"}},"outputId":"cdcbf573-0359-4bcb-844a-562398d3fa0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dados importados com sucesso\n","\n","Qtd. de sentenças: 8766\n","\n","Primeiras sentenças: \n","\n","\n","\n","O ano era 1978. \n","\n","\n","As salas de cinema de todo o mundo exibiam uma produção do diretor Joe Dante em que um cardume de piranhas escapava de um laboratório militar e atacava participantes de um festival aquático. \n","\n","\n","Quase 30 anos depois, banhistas assustados estão se afastando do principal balneário de Uruguaiana, na Fronteira Oeste. \n","\n","\n","Mais de 20 pessoas foram mordidas por palometas (Serrasalmus spilopleura, espécie de piranha) que vivem nas águas da barragem Sanchuri, na margem da Br-472, a 40 quilômetros da cidade. \n","\n","\n","-- Os ataques se tornaram mais freqüentes. \n","\n","\n","Por isso, aconselhamos mais cautela -- diz o subprefeito Nei Pinto. \n","\n","\n","As mordidas em pés e canelas de banhistas não são novidade. \n","\n","\n","O que chamou a atenção das autoridades foi o aumento no número de ataques em relação aos outros anos. \n","\n","\n","-- Um amigo meu ficou sem a ponta de um dedo -- conta o adolescente Lindomar Menezes, 15 anos. \n","\n","\n","Ele e amigos, como Giovane Silva Ferreira, 13 anos, passam as tardes pescando o peixe, depois levado para uma associação de artesãos que faz o curtimento da pele do animal. \n"]}],"source":["def import_dataset(path):\n","\n","    ########################## COMPLETE O CÓDIGO AQUI  ########################\n","\n","    df = pd.read_csv(path, sep = \"\\t\")\n","\n","    # elimina os textos duplicados\n","    df = df.dropna(subset=['sentence_text'])\n","\n","    # elimina os textos duplicados\n","    df = df.drop_duplicates(subset=['sentence_text'])\n","\n","    dataset = df['sentence_text'].values\n","\n","    ##########################################################################\n","\n","    return dataset\n","\n","# descompacta a base de dados de notícias\n","z = ZipFile('dados/porsimplessent-master.zip', 'r')\n","z.extractall('dados/')\n","z.close()\n","\n","# importa a base de dados de noticias falsas\n","dataset = import_dataset('dados/porsimplessent-master/porsimples/porsimples_sentences.tsv')\n","print('Dados importados com sucesso')\n","\n","print('\\nQtd. de sentenças: %d' %len(dataset))\n","\n","print('\\nPrimeiras sentenças: \\n')\n","for sent in dataset[0:10]:\n","    print('\\n')\n","    print(sent)"]},{"cell_type":"markdown","metadata":{"id":"ghIDiUdbfQEE"},"source":["Agora, vamos tratar os textos importados da base de dados. Como o texto está na língua portuguesa, devemos usar uma função de estemização apropriada para a língua portuguesa. Iremos também remover os acentos das palavras.\n","\n","Usaremos alguns módulos da biblioteca [NLTK](https://www.nltk.org/). Para que eles possam ser executados, é necessário fazer o download das bases de dados e pacotes complementares usados pela biblioteca. Para fazer o download de todas os pacotes, use o script `nltk.download('all')`. Para fazer o download apenas do pacote de stopwords e de stemming, use, respecitivamente: `nltk.download('stopwords')` e `nltk.download('rslp')`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2UXYNj2fQEE","executionInfo":{"status":"ok","timestamp":1709775520427,"user_tz":180,"elapsed":7182,"user":{"displayName":"Renato Moraes Silva","userId":"07624830346914349358"}},"outputId":"21d884e7-5768-4733-9cab-ef434c4a89e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Antes do preprocessamento: \n"," Quase 30 anos depois, banhistas assustados estão se afastando do principal balneário de Uruguaiana, na Fronteira Oeste. \n","\n","Depois do preprocessamento: \n"," ['quase', '30', 'anos', 'depois,', 'banhistas', 'assustados', 'estao', 'se', 'afastando', 'do', 'principal', 'balneario', 'de', 'uruguaiana,', 'na', 'fronteira', 'oeste.', '']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n"]}],"source":["!pip install nltk\n","\n","import nltk\n","import unicodedata # sera usada para remover acentos dos documentos em lingua portuguesa\n","\n","# Download the stopwords corpus\n","nltk.download('stopwords')\n","\n","# Download the RSLPStemmer\n","nltk.download('rslp')\n","\n","from nltk.stem import RSLPStemmer # para fazer a estemização em documentos da lingua portuguesa\n","\n","def preprocessing_portuguese(text, stemming = False, stopwords = False):\n","    \"\"\"\n","    Funcao usada para tratar textos escritos na lingua portuguesa\n","\n","    Parametros:\n","        text: variavel do tipo string que contem o texto que devera ser tratado\n","\n","        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n","\n","        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n","    \"\"\"\n","\n","\n","    # Lower case\n","    text = text.lower()\n","\n","    # remove os acentos das palavras\n","    nfkd_form = unicodedata.normalize('NFKD', text)\n","    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n","\n","    # remove tags HTML\n","    regex = re.compile('<[^<>]+>')\n","    text = re.sub(regex, \" \", text)\n","\n","    # normaliza as URLs\n","    regex = re.compile('(http|https)://[^\\s]*')\n","    text = re.sub(regex, \"<URL>\", text)\n","\n","    # normaliza emails\n","    regex = re.compile('[^\\s]+@[^\\s]+')\n","    text = re.sub(regex, \"<EMAIL>\", text)\n","\n","    # converte todos os caracteres não-alfanuméricos em espaço\n","    regex = re.compile('[^A-Za-z0-9]+')\n","    text = re.sub(regex, \" \", text)\n","\n","    # normaliza os numeros\n","    regex = re.compile('[0-9]+.[0-9]+')\n","    text = re.sub(regex, \"NUMERO\", text)\n","\n","    # normaliza os numeros\n","    regex = re.compile('[0-9]+,[0-9]+')\n","    text = re.sub(regex, \"NUMERO\", text)\n","\n","    # normaliza os numeros\n","    regex = re.compile('[0-9]+')\n","    text = re.sub(regex, \"NUMERO\", text)\n","\n","    ########################## COMPLETE O CÓDIGO AQUI  ########################\n","\n","\n","\n","\n","\n","\n","\n","\n","    ##########################################################################\n","\n","    return text\n","\n","exemplo_noticia = 'Quase 30 anos depois, banhistas assustados estão se afastando do principal balneário de Uruguaiana, na Fronteira Oeste. '\n","print('Antes do preprocessamento: \\n', exemplo_noticia)\n","\n","# executa a função de pré-processsamento para tratar a amostra de texto\n","exemplo_noticia = preprocessing_portuguese(exemplo_noticia, stemming = True, stopwords = True)\n","\n","print('\\nDepois do preprocessamento: \\n', exemplo_noticia)"]},{"cell_type":"markdown","metadata":{"id":"FzZl1IxXfQEF"},"source":["Iremos aplicar a pré-processamento em todos os documentos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDQJ0CB8fQEF"},"outputs":[],"source":["########################## COMPLETE O CÓDIGO AQUI  ########################\n","\n","\n","\n","\n","\n","\n","\n","##########################################################################\n","\n","print(\"\\n\\nPrimeira amostra\")\n","print(dataset2[10])"]},{"cell_type":"markdown","metadata":{"id":"wMzk8s_CfQEG"},"source":["Iremos dar uma análisada na base de dados usando uma nuvem de palavras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZF_MLzMfQEG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","!pip install wordcloud\n","\n","from wordcloud import WordCloud\n","\n","########################## COMPLETE O CÓDIGO AQUI  ########################\n","\n","\n","\n","\n","\n","\n","\n","\n","##########################################################################\n","\n","# Plote a nuvem de palavras\n","plt.imshow(wc)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PEtZesa8fQEG"},"source":["Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V0_DJScfQEG"},"outputs":[],"source":["\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JZ9go75OfQEH"},"source":["Vamos converter para TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2QoOJZzfQEH"},"outputs":[],"source":["\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rpIEYh5JfQEH"},"source":["Vamos converter para binário"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEzTsqfwfQEH"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lnawDIjHfQEH"},"source":["Busque os documentos mais relevantes para o termo \"praia\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgpFUmTkfQEI"},"outputs":[],"source":["\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}