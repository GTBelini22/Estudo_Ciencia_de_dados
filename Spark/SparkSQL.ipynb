{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DoC24P6NmAuN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando o Spark"
      ],
      "metadata": {
        "id": "DoC24P6NmAuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NiqW9-xlnjc",
        "outputId": "33b58200-8119-4c4b-c5d3-5593e8c3abc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark #==3.3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5NeVueloGu2",
        "outputId": "ed46ca57-78e1-4b3b-d6ae-c8d5e3e774b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 21:50:45--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  51.4MB/s    in 0.3s    \n",
            "\n",
            "2024-02-27 21:50:46 (51.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iniciar Sessão Spark"
      ],
      "metadata": {
        "id": "8gIqFurprOZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# ConfigureSparkUI\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "sc.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder                  # Método da classe que constrói a sessão spark\n",
        "      .appName(\"Meu Primeiro App Spark\")  # Nome do App Spark\n",
        "      .getOrCreate())                     # Verifica se há uma sessão ativa, e se não há, cria uma nova sessão\n"
      ],
      "metadata": {
        "id": "vcytxNzMzrnX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ca174529-cc2d-4c93-e3cf-390b5fb29d45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Meu Primeiro App Spark, master=local[*]) created by getOrCreate at <ipython-input-3-0e8e82901c43>:12 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0e8e82901c43>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ConfigureSparkUI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.ui.port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4050'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Meu Primeiro App Spark, master=local[*]) created by getOrCreate at <ipython-input-3-0e8e82901c43>:12 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "id": "tRgaHosSo6PN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL"
      ],
      "metadata": {
        "id": "9mPHkAtuYNO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "\n",
        "caminho_json = \"/content/pix_transactions.json\"\n",
        "\n",
        "schema_remetente_destinatario = StructType([\n",
        "    StructField('nome', StringType()),\n",
        "    StructField('banco', StringType()),\n",
        "    StructField('tipo', StringType()),\n",
        "])\n",
        "\n",
        "\n",
        "eschema_pix = StructType([\n",
        "    StructField('chave_pix', StringType()),\n",
        "    StructField('fraude', IntegerType()),\n",
        "    StructField('id_transacao', IntegerType()),\n",
        "    StructField('remetente', schema_remetente_destinatario),\n",
        "    StructField('destinatario', schema_remetente_destinatario),\n",
        "    StructField('transaction_date', TimestampType()),\n",
        "    StructField('valor', DoubleType())\n",
        "])\n",
        "\n",
        "\n",
        "spark.read.json(\n",
        "    path=caminho_json,\n",
        "    schema=eschema_pix,\n",
        "    timestampFormat=\"yyyy-MM-dd\"\n",
        ").createOrReplaceTempView('transacoes_pix')\n",
        "\n",
        "\n",
        "df = spark.read.json(\n",
        "    path=caminho_json,\n",
        "    schema=eschema_pix,\n",
        "    timestampFormat=\"yyyy-MM-dd\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Tww10QvPhTqx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui podemos escrever qualquer query\n",
        "spark.sql('select * from transacoes_pix limit 10').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUXrdkowhrdA",
        "outputId": "e35b36f1-9136-4110-e5a6-c4e35dd04a50"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+------------+--------------------+--------------------+-------------------+-------+\n",
            "|chave_pix|fraude|id_transacao|           remetente|        destinatario|   transaction_date|  valor|\n",
            "+---------+------+------------+--------------------+--------------------+-------------------+-------+\n",
            "|      cpf|     0|        1000|{Jonathan Gonsalv...|{Gabriel Cunha, I...|2022-03-19 00:00:00|   7.05|\n",
            "|aleatoria|     0|        1001|{Jonathan Gonsalv...|{Diego Souza, XP,...|2021-01-26 00:00:00|  37.28|\n",
            "|aleatoria|     0|        1002|{Jonathan Gonsalv...|{Nicole Nunes, BT...|2022-05-31 00:00:00| 282.73|\n",
            "|aleatoria|     0|        1003|{Jonathan Gonsalv...|{Maria Fernanda C...|2022-07-04 00:00:00|8447.92|\n",
            "|aleatoria|     0|        1004|{Jonathan Gonsalv...|{Isabel Silva, C6...|2021-09-11 00:00:00|  58.51|\n",
            "|  celular|     0|        1005|{Jonathan Gonsalv...|{Anthony Carvalho...|2022-02-11 00:00:00|6655.12|\n",
            "|      cpf|     0|        1006|{Jonathan Gonsalv...|{Eloah Monteiro, ...|2022-05-10 00:00:00|9912.25|\n",
            "|aleatoria|     0|        1007|{Jonathan Gonsalv...|{Sophie Rocha, BT...|2022-08-28 00:00:00|8212.91|\n",
            "|      cpf|     0|        1008|{Jonathan Gonsalv...|{Pietro Ribeiro, ...|2022-03-23 00:00:00|  91.71|\n",
            "|      cpf|     0|        1009|{Jonathan Gonsalv...|{Eloah Teixeira, ...|2021-09-18 00:00:00|  44.29|\n",
            "+---------+------+------------+--------------------+--------------------+-------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testando para ver o que é mais facil\n",
        "group_sql = spark.sql('select chave_pix, count(*) from transacoes_pix group by chave_pix ')"
      ],
      "metadata": {
        "id": "ALLuPoVFjOWc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_df = df.groupBy('chave_pix').count()"
      ],
      "metadata": {
        "id": "SSKAngutjjWV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql.explain()\n",
        "\n",
        "group_df.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRHFTtCQktyt",
        "outputId": "5293e75b-b881-45e2-b952-2486308ba639"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix#338], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix#338, 200), ENSURE_REQUIREMENTS, [plan_id=123]\n",
            "      +- HashAggregate(keys=[chave_pix#338], functions=[partial_count(1)])\n",
            "         +- FileScan json [chave_pix#338] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/content/pix_transactions.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[chave_pix#352], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(chave_pix#352, 200), ENSURE_REQUIREMENTS, [plan_id=136]\n",
            "      +- HashAggregate(keys=[chave_pix#352], functions=[partial_count(1)])\n",
            "         +- FileScan json [chave_pix#352] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/content/pix_transactions.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_sql.show()\n",
        "\n",
        "group_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAZkjvFGk-so",
        "outputId": "3a68843c-088c-4b55-cadb-345008d03752"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|chave_pix|count(1)|\n",
            "+---------+--------+\n",
            "|aleatoria|   25045|\n",
            "|  celular|   24841|\n",
            "|    email|   24935|\n",
            "|      cpf|   25179|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+-----+\n",
            "|chave_pix|count|\n",
            "+---------+-----+\n",
            "|aleatoria|25045|\n",
            "|  celular|24841|\n",
            "|    email|24935|\n",
            "|      cpf|25179|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "      select\n",
        "      chave_pix,\n",
        "      sum(valor) as valor_total\n",
        "      from transacoes_pix\n",
        "      group by chave_pix\n",
        "      order by valor_total desc\n",
        "    \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml3jlerrlPpa",
        "outputId": "6880fa8b-0826-46a1-9db7-e644ff2f08f6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|chave_pix|         valor_total|\n",
            "+---------+--------------------+\n",
            "|aleatoria| 3.059806293100002E8|\n",
            "|  celular| 3.018847117200015E8|\n",
            "|      cpf| 3.007901403699987E8|\n",
            "|    email|2.9592901058000124E8|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    select\n",
        "    chave_pix,\n",
        "    count(*) as numero_total\n",
        "    from transacoes_pix\n",
        "    where valor>10000\n",
        "    group by chave_pix\n",
        "    order by numero_total desc\n",
        "    \"\"\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRRyNu1-l5ow",
        "outputId": "2dd45d08-044e-4171-ef78-31a9ced29ab6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|chave_pix|numero_total|\n",
            "+---------+------------+\n",
            "|aleatoria|        5032|\n",
            "|      cpf|        4950|\n",
            "|  celular|        4922|\n",
            "|    email|        4830|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}